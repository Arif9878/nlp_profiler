{
 "nbformat": 4,
 "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "better_nlp_summarisers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFyUOrS1fczL"
   },
   "source": [
    "# Better NLP\n",
    "\n",
    "This is a wrapper program/library that encapsulates a couple of NLP libraries that are popular among the AI and ML communities.\n",
    "\n",
    "Examples have been used to illustrate the usage as much as possible. Not all the APIs of the underlying libraries have been covered.\n",
    "\n",
    "The idea is to keep the API language as high-level as possible, so its easier to use and stays human-readable.\n",
    "\n",
    "Libraries / frameworks covered:\n",
    "\n",
    "- nltk [site](http://www.nltk.org/) | [docs](https://buildmedia.readthedocs.org/media/pdf/nltk/latest/nltk.pdf)\n",
    "- numpy [site](https://www.numpy.org/) | [docs](https://docs.scipy.org/doc/)\n",
    "- networkx [site](https://networkx.github.io/) | [docs](https://networkx.github.io/documentation/stable/index.html)\n",
    "\n",
    "See [https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp](https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp) for more details.\n",
    "\n",
    "### This notebook will demonstrate the below NLP features / functionalities, using the above mentioned libraries\n",
    "\n",
    "- Cosine similarity summarisation technique (extractive summarisation)\n",
    "- Vertex ranking algorithm summarisation technique\n",
    "- Build a simple text summarisation tool using NLTK\n",
    "- Summarisation 4 (TODO)\n",
    "- Summarisation 5 (TODO)\n",
    "\n",
    "_Summarisation can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMTAIhz53w8G"
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Understand Text Summarization and create your own summarizer in python](https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70) or [An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)\n",
    "- [Beyond bag of words: Using PyTextRank to find Phrases and Summarize text](https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5)\n",
    "- [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f)\n",
    "- [Summarise Text with TFIDF in Python 1/2](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2/2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
    "- [How to Make a Text Summarizer - Intro to Deep Learning #10 by Siraj Raval](https://www.youtube.com/watch?v=ogrJaOIuBx4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lre8GErufczN"
   },
   "source": [
    "#### Setup and installation ( optional )\n",
    "\n",
    "In case, this notebook is running in a local environment (Linux/MacOS) or _Google Colab_ environment and in case it does not have the necessary dependencies installed then please execute the steps in the next section.\n",
    "\n",
    "Otherwise, please SKIP to the **Install Spacy model ( NOT optional )** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5927
    },
    "colab_type": "code",
    "id": "QJuCUOMOfczO",
    "outputId": "018e9102-de25-4fbe-b5b3-919c58644f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "dpkg is already the newest version (1.18.25).\n",
      "dpkg set to manually installed.\n",
      "The following NEW packages will be installed:\n",
      "  apt-utils dselect libapt-inst2.0\n",
      "0 upgraded, 3 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 1888 kB of archives.\n",
      "After this operation, 4168 kB of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 libapt-inst2.0 amd64 1.4.9 [192 kB]\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 apt-utils amd64 1.4.9 [410 kB]\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 dselect amd64 1.18.25 [1285 kB]\n",
      "Fetched 1888 kB in 2s (926 kB/s)\n",
      "Selecting previously unselected package libapt-inst2.0:amd64.\n",
      "(Reading database ... 36312 files and directories currently installed.)\n",
      "Preparing to unpack .../libapt-inst2.0_1.4.9_amd64.deb ...\n",
      "Unpacking libapt-inst2.0:amd64 (1.4.9) ...\n",
      "Selecting previously unselected package apt-utils.\n",
      "Preparing to unpack .../apt-utils_1.4.9_amd64.deb ...\n",
      "Unpacking apt-utils (1.4.9) ...\n",
      "Selecting previously unselected package dselect.\n",
      "Preparing to unpack .../dselect_1.18.25_amd64.deb ...\n",
      "Unpacking dselect (1.18.25) ...\n",
      "Setting up libapt-inst2.0:amd64 (1.4.9) ...\n",
      "Setting up apt-utils (1.4.9) ...\n",
      "Setting up dselect (1.18.25) ...\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\n",
      "OSTYPE=linux-gnu\n",
      "Library source found\n",
      "Detected OS: linux\n",
      "Please check if you fulfill the requirements mentioned in the README file.\n",
      "Ign:1 http://deb.debian.org/debian stretch InRelease\n",
      "Get:2 http://security.debian.org/debian-security stretch/updates InRelease [94.3 kB]\n",
      "Get:3 http://deb.debian.org/debian stretch-updates InRelease [91.0 kB]\n",
      "Get:4 https://deb.nodesource.com/node_8.x stretch InRelease [4620 B]\n",
      "Get:5 http://deb.debian.org/debian stretch Release [118 kB]\n",
      "Get:6 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [500 kB]\n",
      "Get:7 https://deb.nodesource.com/node_8.x stretch/main amd64 Packages [1009 B]\n",
      "Get:8 http://deb.debian.org/debian stretch-updates/main amd64 Packages.diff/Index [11.6 kB]\n",
      "Get:9 http://deb.debian.org/debian stretch Release.gpg [2365 B]\n",
      "Get:10 http://deb.debian.org/debian stretch-updates/main amd64 Packages 2019-05-18-1409.05.pdiff [51 B]\n",
      "Get:11 http://deb.debian.org/debian stretch-updates/main amd64 Packages 2019-07-08-0821.07.pdiff [445 B]\n",
      "Get:12 http://deb.debian.org/debian stretch-updates/main amd64 Packages 2019-08-14-2019.27.pdiff [339 B]\n",
      "Get:13 http://deb.debian.org/debian stretch-updates/main amd64 Packages 2019-09-18-2012.01.pdiff [337 B]\n",
      "Get:13 http://deb.debian.org/debian stretch-updates/main amd64 Packages 2019-09-18-2012.01.pdiff [337 B]\n",
      "Get:14 http://deb.debian.org/debian stretch/main amd64 Packages [7086 kB]\n",
      "Fetched 7910 kB in 10s (733 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.52.1-5+deb9u9).\n",
      "liblapack-dev is already the newest version (3.7.0-2).\n",
      "pkg-config is already the newest version (0.29-4+b1).\n",
      "wget is already the newest version (1.18-5+deb9u3).\n",
      "The following additional packages will be installed:\n",
      "  libavutil-dev libavutil55 libswscale4\n",
      "The following packages will be upgraded:\n",
      "  libavutil-dev libavutil55 libswscale-dev libswscale4\n",
      "4 upgraded, 0 newly installed, 0 to remove and 64 not upgraded.\n",
      "Need to get 949 kB of archives.\n",
      "After this operation, 123 kB disk space will be freed.\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 libswscale-dev amd64 7:3.2.14-1~deb9u1 [213 kB]\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 libavutil-dev amd64 7:3.2.14-1~deb9u1 [318 kB]\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 libavutil55 amd64 7:3.2.14-1~deb9u1 [221 kB]\n",
      "Get:4 http://deb.debian.org/debian stretch/main amd64 libswscale4 amd64 7:3.2.14-1~deb9u1 [196 kB]\n",
      "Fetched 949 kB in 1s (888 kB/s)\n",
      "(Reading database ... 36528 files and directories currently installed.)\n",
      "Preparing to unpack .../libswscale-dev_7%3a3.2.14-1~deb9u1_amd64.deb ...\n",
      "Unpacking libswscale-dev:amd64 (7:3.2.14-1~deb9u1) over (7:3.2.12-1~deb9u1) ...\n",
      "Preparing to unpack .../libavutil-dev_7%3a3.2.14-1~deb9u1_amd64.deb ...\n",
      "Unpacking libavutil-dev:amd64 (7:3.2.14-1~deb9u1) over (7:3.2.12-1~deb9u1) ...\n",
      "Preparing to unpack .../libavutil55_7%3a3.2.14-1~deb9u1_amd64.deb ...\n",
      "Unpacking libavutil55:amd64 (7:3.2.14-1~deb9u1) over (7:3.2.12-1~deb9u1) ...\n",
      "Preparing to unpack .../libswscale4_7%3a3.2.14-1~deb9u1_amd64.deb ...\n",
      "Unpacking libswscale4:amd64 (7:3.2.14-1~deb9u1) over (7:3.2.12-1~deb9u1) ...\n",
      "Setting up libavutil55:amd64 (7:3.2.14-1~deb9u1) ...\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\n",
      "Setting up libswscale4:amd64 (7:3.2.14-1~deb9u1) ...\n",
      "Setting up libavutil-dev:amd64 (7:3.2.14-1~deb9u1) ...\n",
      "Setting up libswscale-dev:amd64 (7:3.2.14-1~deb9u1) ...\n",
      "Processing triggers for libc-bin (2.24-11+deb9u4) ...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "zip is already the newest version (3.0-11+b1).\n",
      "The following additional packages will be installed:\n",
      "  vim-common vim-runtime\n",
      "Suggested packages:\n",
      "  ctags vim-doc vim-scripts\n",
      "The following packages will be upgraded:\n",
      "  vim vim-common vim-runtime\n",
      "3 upgraded, 0 newly installed, 0 to remove and 61 not upgraded.\n",
      "Need to get 6602 kB of archives.\n",
      "After this operation, 4096 B of additional disk space will be used.\n",
      "Get:1 http://deb.debian.org/debian stretch/main amd64 vim amd64 2:8.0.0197-4+deb9u3 [1034 kB]\n",
      "Get:2 http://deb.debian.org/debian stretch/main amd64 vim-runtime all 2:8.0.0197-4+deb9u3 [5409 kB]\n",
      "Get:3 http://deb.debian.org/debian stretch/main amd64 vim-common all 2:8.0.0197-4+deb9u3 [159 kB]\n",
      "Fetched 6602 kB in 7s (932 kB/s)\n",
      "(Reading database ... 36528 files and directories currently installed.)\n",
      "Preparing to unpack .../vim_2%3a8.0.0197-4+deb9u3_amd64.deb ...\n",
      "Unpacking vim (2:8.0.0197-4+deb9u3) over (2:8.0.0197-4+deb9u1) ...\n",
      "Preparing to unpack .../vim-runtime_2%3a8.0.0197-4+deb9u3_all.deb ...\n",
      "Unpacking vim-runtime (2:8.0.0197-4+deb9u3) over (2:8.0.0197-4+deb9u1) ...\n",
      "Preparing to unpack .../vim-common_2%3a8.0.0197-4+deb9u3_all.deb ...\n",
      "Unpacking vim-common (2:8.0.0197-4+deb9u3) over (2:8.0.0197-4+deb9u1) ...\n",
      "Processing triggers for mime-support (3.60) ...\n",
      "Setting up vim-common (2:8.0.0197-4+deb9u3) ...\n",
      "Setting up vim-runtime (2:8.0.0197-4+deb9u3) ...\n",
      "Processing triggers for hicolor-icon-theme (0.15-1) ...\n",
      "Setting up vim (2:8.0.0197-4+deb9u3) ...\n",
      "\n",
      "## Installing the NodeSource Node.js 8.x LTS Carbon repo...\n",
      "\n",
      "\n",
      "## Populating apt-get cache...\n",
      "\n",
      "+ apt-get update\n",
      "Hit:1 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Ign:2 http://deb.debian.org/debian stretch InRelease\n",
      "Hit:3 http://deb.debian.org/debian stretch-updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian stretch Release\n",
      "Hit:5 https://deb.nodesource.com/node_8.x stretch InRelease\n",
      "Reading package lists...\n",
      "\n",
      "## Confirming \"stretch\" is supported...\n",
      "\n",
      "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_8.x/dists/stretch/Release'\n",
      "\n",
      "## Adding the NodeSource signing key to your keyring...\n",
      "\n",
      "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | apt-key add -\n",
      "OK\n",
      "\n",
      "## Creating apt sources list file for the NodeSource Node.js 8.x LTS Carbon repo...\n",
      "\n",
      "+ echo 'deb https://deb.nodesource.com/node_8.x stretch main' > /etc/apt/sources.list.d/nodesource.list\n",
      "+ echo 'deb-src https://deb.nodesource.com/node_8.x stretch main' >> /etc/apt/sources.list.d/nodesource.list\n",
      "\n",
      "## Running `apt-get update` for you...\n",
      "\n",
      "+ apt-get update\n",
      "Hit:1 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Ign:2 http://deb.debian.org/debian stretch InRelease\n",
      "Hit:3 http://deb.debian.org/debian stretch-updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian stretch Release\n",
      "Hit:5 https://deb.nodesource.com/node_8.x stretch InRelease\n",
      "Reading package lists...\n",
      "\n",
      "## Run `sudo apt-get install -y nodejs` to install Node.js 8.x LTS Carbon and npm\n",
      "## You may also need development tools to build native addons:\n",
      "     sudo apt-get install gcc g++ make\n",
      "## To install the Yarn package manager, run:\n",
      "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\n",
      "     echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
      "     sudo apt-get update && sudo apt-get install yarn\n",
      "\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be upgraded:\n",
      "  nodejs\n",
      "1 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
      "Need to get 13.6 MB of archives.\n",
      "After this operation, 16.4 kB of additional disk space will be used.\n",
      "Get:1 https://deb.nodesource.com/node_8.x stretch/main amd64 nodejs amd64 8.16.1-1nodesource1 [13.6 MB]\n",
      "Fetched 13.6 MB in 14s (938 kB/s)\n",
      "(Reading database ... 36528 files and directories currently installed.)\n",
      "Preparing to unpack .../nodejs_8.16.1-1nodesource1_amd64.deb ...\n",
      "Detected old npm client, removing...\n",
      "Unpacking nodejs (8.16.1-1nodesource1) over (8.16.0-1nodesource1) ...\n",
      "Setting up nodejs (8.16.1-1nodesource1) ...\n",
      "Install components using python and pip\n",
      "/usr/bin/npm -> /usr/lib/node_modules/npm/bin/npm-cli.js\n",
      "/usr/bin/npx -> /usr/lib/node_modules/npm/bin/npx-cli.js\n",
      "+ npm@6.12.0\n",
      "added 62 packages from 18 contributors, removed 19 packages and updated 70 packages in 30.635s\n",
      "6.12.0\n",
      "Python 3.7.3\n",
      "pip 19.0.3 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (2.1.4)\n",
      "Requirement already satisfied: textacy in /usr/local/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: pytextrank in /root/.local/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: nltk in /root/.local/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (7.0.4)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/site-packages (from spacy) (0.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.16.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.9.0.1)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.7/site-packages (from textacy) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn<0.21.0,>=0.17.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.7/site-packages (from textacy) (4.32.1)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.7/site-packages (from textacy) (0.9.5)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from textacy) (1.2.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/site-packages (from textacy) (2.3)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: datasketch in /root/.local/lib/python3.7/site-packages (from pytextrank) (1.4.3)\n",
      "Requirement already satisfied: statistics in /root/.local/lib/python3.7/site-packages (from pytextrank) (1.0.3.5)\n",
      "Requirement already satisfied: graphviz in /root/.local/lib/python3.7/site-packages (from pytextrank) (0.10.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (40.8.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/site-packages (from networkx>=1.11->textacy) (4.4.0)\n",
      "Requirement already satisfied: redis>=2.10.0 in /root/.local/lib/python3.7/site-packages (from datasketch->pytextrank) (3.2.1)\n",
      "Requirement already satisfied: docutils>=0.3 in /root/.local/lib/python3.7/site-packages (from statistics->pytextrank) (0.14)\n",
      "Requirement already satisfied: jupyterlab in /root/.local/lib/python3.7/site-packages (0.35.6)\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /root/.local/lib/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: jupyterlab-server<0.3.0,>=0.2.0 in /root/.local/lib/python3.7/site-packages (from jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.3.1 in /root/.local/lib/python3.7/site-packages (from jupyterlab) (5.7.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/site-packages (from pandas) (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /root/.local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /root/.local/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /root/.local/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/.local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/.local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/site-packages (from jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (3.0.1)\n",
      "Requirement already satisfied: nbformat in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.3.2)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.2.4)\n",
      "Requirement already satisfied: jinja2 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (2.10.1)\n",
      "Requirement already satisfied: ipython-genutils in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: prometheus-client in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: nbconvert in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.5.0)\n",
      "Requirement already satisfied: Send2Trash in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.8.2)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (6.0.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (18.0.1)\n",
      "Requirement already satisfied: ipykernel in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6.0->jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (0.15.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6.0->jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (19.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from traitlets>=4.2.1->notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /root/.local/lib/python3.7/site-packages (from jinja2->notebook>=4.3.1->jupyterlab) (1.1.1)\n",
      "Requirement already satisfied: testpath in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.4.2)\n",
      "Requirement already satisfied: pygments in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (2.4.0)\n",
      "Requirement already satisfied: bleach in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (3.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (1.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.3)\n",
      "Requirement already satisfied: defusedxml in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /root/.local/lib/python3.7/site-packages (from terminado>=0.8.1->notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /root/.local/lib/python3.7/site-packages (from ipykernel->notebook>=4.3.1->jupyterlab) (7.5.0)\n",
      "Requirement already satisfied: webencodings in /root/.local/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.3.1->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: backcall in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.13.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (4.7.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (2.0.9)\n",
      "Requirement already satisfied: pickleshare in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.7.5)\n",
      "Requirement already satisfied: parso>=0.3.0 in /root/.local/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.4.0)\n",
      "Requirement already satisfied: wcwidth in /root/.local/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.1.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 30 ms, total: 40 ms\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "apt-get install apt-utils dselect dpkg\n",
    "\n",
    "echo \"OSTYPE=$OSTYPE\"\n",
    "if [[ \"$OSTYPE\" == \"cygwin\" ]] || [[ \"$OSTYPE\" == \"msys\" ]] ; then\n",
    "    echo \"Windows or Windows-like environment detected, script not tested, and may not work.\"\n",
    "    echo \"Try installing the components mention in the install-[ostype].sh scripts manually.\"\n",
    "    echo \"Or try running under CGYWIN or git-bash.\"\n",
    "    echo \"If successfully installed, please contribute back with the solution via a pull request, to https://github.com/neomatrix369/awesome-ai-ml-dl/\"\n",
    "    echo \"Please give the file a good name, i.e. install-windows.sh or install-windows.bat depending on what kind of script you end up writing\"\n",
    "    exit 0\n",
    "elif [[ \"$OSTYPE\" == \"linux-gnu\" ]] || [[ \"$OSTYPE\" == \"linux\" ]]; then\n",
    "    TARGET_OS=\"linux\"\n",
    "else\n",
    "    TARGET_OS=\"macos\"\n",
    "fi\n",
    "\n",
    "if [[ -e ../../library/org/neomatrix369 ]]; then\n",
    "  echo \"Library source found\"\n",
    "  \n",
    "  cd ../../build\n",
    "  \n",
    "  echo \"Detected OS: ${TARGET_OS}\"\n",
    "  ./install-${TARGET_OS}.sh || true\n",
    "else\n",
    "  if [[ -e awesome-ai-ml-dl/examples/better-nlp/library ]]; then\n",
    "     echo \"Library source found\"\n",
    "  else\n",
    "     git clone \"https://github.com/neomatrix369/awesome-ai-ml-dl\"\n",
    "  fi\n",
    "\n",
    "  echo \"Library source exists\"\n",
    "  cd awesome-ai-ml-dl/examples/better-nlp/build\n",
    "\n",
    "  echo \"Detected OS: ${TARGET_OS}\"\n",
    "  ./install-${TARGET_OS}.sh || true \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPLdwvt63w8R"
   },
   "source": [
    "#### Install Spacy model ( NOT optional )\n",
    "\n",
    "Install the large English language model for spaCy - will be needed for the examples in this notebooks.\n",
    "\n",
    "**Note:** from observation it appears that spaCy model should be installed towards the end of the installation process, it avoid errors when running programs using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dJrJ54a3w8S",
    "outputId": "3da4300b-89a8-43e3-e989-fa7859ecdfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz (826.9MB)\n",
      "Installing collected packages: en-core-web-lg\n",
      "  Running setup.py install for en-core-web-lg: started\n",
      "    Running setup.py install for en-core-web-lg: finished with status 'done'\n",
      "Successfully installed en-core-web-lg-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/site-packages/en_core_web_lg -->\n",
      "/usr/local/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 20 ms, total: 20 ms\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "python -m spacy download en_core_web_lg\n",
    "python -m spacy link en_core_web_lg en || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwXgEdM8oeUv"
   },
   "source": [
    "## Examples of various summarisation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AX1pZlKofczb"
   },
   "source": [
    "### 1. Cosine similarity summarisation technique (extractive summarisation)\n",
    "\n",
    "**Abstractive Summarization:** Abstractive methods select words based on semantic understanding, even those words did not appear in the source documents. It aims at producing important material in a new way. They interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text.\n",
    "\n",
    "**Flow:** Input document → understand context → semantics → create own summary\n",
    "\n",
    "**Extractive Summarization:** Extractive methods attempt to summarize articles by selecting a subset of words that retain the most important points.\n",
    "\n",
    "**Flow:** Input document → sentences similarity → weight sentences → select sentences with higher rank\n",
    "\n",
    "**Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. Angle will be 0 if sentences are similar and tend towards 90 as they begin to differ.\n",
    "\n",
    "Inspired by Praveen Dubey the author of https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "\n",
    "or see [Understand Text Summarization and create your own summarizer in python](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yjan7P5_3w8Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of Python is 64 bits.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../library')\n",
    "sys.path.insert(0, './awesome-ai-ml-dl/examples/better-nlp/library')\n",
    "\n",
    "from org.neomatrix369.better_nlp import BetterNLP\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpjkLRHe3w8c"
   },
   "outputs": [],
   "source": [
    "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object\n",
    "generic_text=\"\"\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vkYIGs13w8f",
    "outputId": "083398af-ca33-4c1c-f892-e1567de17931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 0.1114656925201416\n",
      "('summarised_text=As part of the program, the Palo Alto giant which wants to '\n",
      " 'expand its reach and is planning to build a strong developer ecosystem in '\n",
      " 'India with the program will set up the core AI infrastructure and IoT Hub '\n",
      " 'for the selected campuses. The company will provide AI development tools and '\n",
      " 'AI services such as SmartSoft Corp. The program is an attempt to ramp up the '\n",
      " 'institutional set-up and build capabilities among the educators to educate '\n",
      " 'the workforce of tomorrow.\" The program aims to build up the cognitive '\n",
      " 'skills and in-depth understanding of developing intelligent cloud connected '\n",
      " 'solutions for applications across industry. The program was developed to '\n",
      " 'provide job ready skills to programmers who wanted to hone their skills in '\n",
      " 'AI and data science with a series of online courses which featured hands-on '\n",
      " 'labs and expert instructors as well. Envisioned as a three-year '\n",
      " 'collaborative program, Smart Colab Program will support around 100 '\n",
      " 'institutions with AI infrastructure, course content and curriculum, '\n",
      " 'developer support, development tools and give students access to cloud and '\n",
      " 'AI services')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text)\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranked_sentences=\n",
      "[   (   0.1005088411163724,\n",
      "        [   'As',\n",
      "            'part',\n",
      "            'of',\n",
      "            'the',\n",
      "            'program,',\n",
      "            'the',\n",
      "            'Palo',\n",
      "            'Alto',\n",
      "            'giant',\n",
      "            'which',\n",
      "            'wants',\n",
      "            'to',\n",
      "            'expand',\n",
      "            'its',\n",
      "            'reach',\n",
      "            'and',\n",
      "            'is',\n",
      "            'planning',\n",
      "            'to',\n",
      "            'build',\n",
      "            'a',\n",
      "            'strong',\n",
      "            'developer',\n",
      "            'ecosystem',\n",
      "            'in',\n",
      "            'India',\n",
      "            'with',\n",
      "            'the',\n",
      "            'program',\n",
      "            'will',\n",
      "            'set',\n",
      "            'up',\n",
      "            'the',\n",
      "            'core',\n",
      "            'AI',\n",
      "            'infrastructure',\n",
      "            'and',\n",
      "            'IoT',\n",
      "            'Hub',\n",
      "            'for',\n",
      "            'the',\n",
      "            'selected',\n",
      "            'campuses']),\n",
      "    (   0.0927534462515247,\n",
      "        [   'The',\n",
      "            'company',\n",
      "            'will',\n",
      "            'provide',\n",
      "            'AI',\n",
      "            'development',\n",
      "            'tools',\n",
      "            'and',\n",
      "            'AI',\n",
      "            'services',\n",
      "            'such',\n",
      "            'as',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.09107393553087705,\n",
      "        [   'The',\n",
      "            'program',\n",
      "            'is',\n",
      "            'an',\n",
      "            'attempt',\n",
      "            'to',\n",
      "            'ramp',\n",
      "            'up',\n",
      "            'the',\n",
      "            'institutional',\n",
      "            'set-up',\n",
      "            'and',\n",
      "            'build',\n",
      "            'capabilities',\n",
      "            'among',\n",
      "            'the',\n",
      "            'educators',\n",
      "            'to',\n",
      "            'educate',\n",
      "            'the',\n",
      "            'workforce',\n",
      "            'of',\n",
      "            'tomorrow.\"',\n",
      "            'The',\n",
      "            'program',\n",
      "            'aims',\n",
      "            'to',\n",
      "            'build',\n",
      "            'up',\n",
      "            'the',\n",
      "            'cognitive',\n",
      "            'skills',\n",
      "            'and',\n",
      "            'in-depth',\n",
      "            'understanding',\n",
      "            'of',\n",
      "            'developing',\n",
      "            'intelligent',\n",
      "            'cloud',\n",
      "            'connected',\n",
      "            'solutions',\n",
      "            'for',\n",
      "            'applications',\n",
      "            'across',\n",
      "            'industry']),\n",
      "    (   0.0891243848923585,\n",
      "        [   'The',\n",
      "            'program',\n",
      "            'was',\n",
      "            'developed',\n",
      "            'to',\n",
      "            'provide',\n",
      "            'job',\n",
      "            'ready',\n",
      "            'skills',\n",
      "            'to',\n",
      "            'programmers',\n",
      "            'who',\n",
      "            'wanted',\n",
      "            'to',\n",
      "            'hone',\n",
      "            'their',\n",
      "            'skills',\n",
      "            'in',\n",
      "            'AI',\n",
      "            'and',\n",
      "            'data',\n",
      "            'science',\n",
      "            'with',\n",
      "            'a',\n",
      "            'series',\n",
      "            'of',\n",
      "            'online',\n",
      "            'courses',\n",
      "            'which',\n",
      "            'featured',\n",
      "            'hands-on',\n",
      "            'labs',\n",
      "            'and',\n",
      "            'expert',\n",
      "            'instructors',\n",
      "            'as',\n",
      "            'well']),\n",
      "    (   0.086410446263451,\n",
      "        [   'Envisioned',\n",
      "            'as',\n",
      "            'a',\n",
      "            'three-year',\n",
      "            'collaborative',\n",
      "            'program,',\n",
      "            'Smart',\n",
      "            'Colab',\n",
      "            'Program',\n",
      "            'will',\n",
      "            'support',\n",
      "            'around',\n",
      "            '100',\n",
      "            'institutions',\n",
      "            'with',\n",
      "            'AI',\n",
      "            'infrastructure,',\n",
      "            'course',\n",
      "            'content',\n",
      "            'and',\n",
      "            'curriculum,',\n",
      "            'developer',\n",
      "            'support,',\n",
      "            'development',\n",
      "            'tools',\n",
      "            'and',\n",
      "            'give',\n",
      "            'students',\n",
      "            'access',\n",
      "            'to',\n",
      "            'cloud',\n",
      "            'and',\n",
      "            'AI',\n",
      "            'services']),\n",
      "    (   0.07940482690749705,\n",
      "        [   'Advanced',\n",
      "            'Program',\n",
      "            'In',\n",
      "            'AI',\n",
      "            'as',\n",
      "            'a',\n",
      "            'learning',\n",
      "            'track',\n",
      "            'open',\n",
      "            'to',\n",
      "            'the',\n",
      "            'public']),\n",
      "    (   0.07552190239284035,\n",
      "        [   'India,',\n",
      "            'said,',\n",
      "            '\"With',\n",
      "            'AI',\n",
      "            'being',\n",
      "            'the',\n",
      "            'defining',\n",
      "            'technology',\n",
      "            'of',\n",
      "            'our',\n",
      "            'time,',\n",
      "            'it',\n",
      "            'is',\n",
      "            'transforming',\n",
      "            'lives',\n",
      "            'and',\n",
      "            'industry',\n",
      "            'and',\n",
      "            'the',\n",
      "            'jobs',\n",
      "            'of',\n",
      "            'tomorrow',\n",
      "            'will',\n",
      "            'require',\n",
      "            'a',\n",
      "            'different',\n",
      "            'skillset']),\n",
      "    (   0.06831027977170921,\n",
      "        [   'This',\n",
      "            'will',\n",
      "            'require',\n",
      "            'more',\n",
      "            'collaborations',\n",
      "            'and',\n",
      "            'training',\n",
      "            'and',\n",
      "            'working',\n",
      "            'with',\n",
      "            'AI']),\n",
      "    (   0.061347193209250084,\n",
      "        [   'announced',\n",
      "            'Smart',\n",
      "            'Colab',\n",
      "            'Program',\n",
      "            'which',\n",
      "            'has',\n",
      "            'been',\n",
      "            'launched',\n",
      "            'to',\n",
      "            'empower',\n",
      "            'the',\n",
      "            'next',\n",
      "            'generation',\n",
      "            'of',\n",
      "            'students',\n",
      "            'with',\n",
      "            'AI-ready',\n",
      "            'skills']),\n",
      "    (   0.057387957742993136,\n",
      "        [   'According',\n",
      "            'to',\n",
      "            'Mark',\n",
      "            'Smith,',\n",
      "            'Country',\n",
      "            'AI',\n",
      "            'Manager,',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.05583463297815709,\n",
      "        [   'That’s',\n",
      "            'why',\n",
      "            'it',\n",
      "            'has',\n",
      "            'become',\n",
      "            'more',\n",
      "            'critical',\n",
      "            'than',\n",
      "            'ever',\n",
      "            'for',\n",
      "            'educational',\n",
      "            'institutions',\n",
      "            'to',\n",
      "            'integrate',\n",
      "            'new',\n",
      "            'cloud',\n",
      "            'and',\n",
      "            'AI',\n",
      "            'technologies']),\n",
      "    (   0.05409704750762544,\n",
      "        [   'Earlier',\n",
      "            'in',\n",
      "            'April',\n",
      "            'this',\n",
      "            'year,',\n",
      "            'the',\n",
      "            'company',\n",
      "            'announced',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.0499849403541697,\n",
      "        [   'In',\n",
      "            'an',\n",
      "            'attempt',\n",
      "            'to',\n",
      "            'build',\n",
      "            'an',\n",
      "            'AI-ready',\n",
      "            'workforce,',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.03824016508117425,\n",
      "        [   'Cognitive',\n",
      "            'Services,',\n",
      "            'Bot',\n",
      "            'Services',\n",
      "            'and',\n",
      "            'Machine',\n",
      "            'Learning',\n",
      "            'Services'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"ranked_sentences=\") \n",
    "pp.pprint(summarised_result['ranked_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7A3p05vfcz0"
   },
   "source": [
    "### 2. Vertex ranking algorithm summarisation technique\n",
    "\n",
    "Using PyTextRank to find Phrases and Summarize text: Multi-word Phrase Extraction and Sentence Extraction for Summarization\n",
    "\n",
    "Inspired by the author of https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5 \n",
    "(Notebook: https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb)\n",
    "\n",
    "Another resource to take a look at: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzEgIzdi3w8j",
    "outputId": "dc771fd8-16ef-487d-f23c-9236d62b97d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sed: can't read /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py: No such file or directory\n",
      "sed: can't read /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 20 ms, total: 80 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "sed -i 's/graph.edge\\[/graph.adj\\[/g' /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "sed -i 's/, parse=True//g' /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "\n",
    "sed -i 's/graph.edge\\[/graph.adj\\[/g' ~/.local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "sed -i 's/, parse=True//g' ~/.local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0by6FnE3w8n"
   },
   "outputs": [],
   "source": [
    "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w44pghUL3w8r",
    "outputId": "885ba416-64b3-48ef-b27c-9b66f2ca069f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to incompatibility between spacy and pytextrank, we are unable to generate a summary\n"
     ]
    }
   ],
   "source": [
    "source_file='source.json'\n",
    "source_json_content='{\"id\":\"777\", \"text\":\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, ''With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That''s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.'' The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"}'\n",
    "with open(source_file, 'w') as f:\n",
    "    f.write(\"%s\" % source_json_content)\n",
    "try:\n",
    "    summarised_result = betterNLP.summarise(source_file, method=\"pytextrank\")\n",
    "\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "    print(\"summarised_text=\",summarised_result['summarised_text'])\n",
    "    print(\"token_ranks=\",summarised_result['token_ranks'])\n",
    "    print(\"key_phrases=\",summarised_result['key_phrases'])\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    betterNLP.show_graph(summarised_result[\"graph\"])\n",
    "except:\n",
    "    print(\"Due to incompatibility between spacy and pytextrank, we are unable to generate a summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2kyTI6bfcz7"
   },
   "source": [
    "### 3. Build a simple text summarisation tool using NLTK\n",
    "\n",
    "Inspired by Wilame Lima Vallantin, the author of [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f).\n",
    "\n",
    "We have to break the text into sentences and tokens, remove stop-words. Tokenise words, calculate word frequency to determine if a word is important on the corpus, using the TF-IDF technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOqeUNkg3w82",
    "outputId": "bfe3346d-713d-4114-b0e8-1a8539895072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 5.4831061363220215\n",
      "summarised_text=\n",
      "[   'Envisioned as a three-year collaborative program, Smart Colab Program '\n",
      "    'will support around 100 institutions with AI infrastructure, course '\n",
      "    'content and curriculum, developer support, development tools and give '\n",
      "    'students access to cloud and AI services.',\n",
      "    'As part of the program, the Palo Alto giant which wants to expand its '\n",
      "    'reach and is planning to build a strong developer ecosystem in India with '\n",
      "    'the program will set up the core AI infrastructure and IoT Hub for the '\n",
      "    'selected campuses.']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'al', 'au', 'bi', 'could', 'de', 'diesis', 'doe', 'dy', 'e', 'ha', 'might', 'mus', 'must', \"n't\", 'need', 'sha', 'un', 'wa', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf\")\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "print(\"summarised_text=\")\n",
    "pp.pprint(summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "important_words=\n",
      "[   ('ai', 0.5370250501018179),\n",
      "    ('program', 0.4130961923860138),\n",
      "    ('build', 0.2065480961930069),\n",
      "    ('skill', 0.2065480961930069),\n",
      "    ('service', 0.2065480961930069),\n",
      "    ('smartsoft', 0.16523847695440552),\n",
      "    ('corp.', 0.16523847695440552),\n",
      "    ('cloud', 0.12392885771580414),\n",
      "    ('attempt', 0.08261923847720276),\n",
      "    ('ai-ready', 0.08261923847720276),\n",
      "    ('workforce', 0.08261923847720276),\n",
      "    ('announced', 0.08261923847720276),\n",
      "    ('smart', 0.08261923847720276),\n",
      "    ('colab', 0.08261923847720276),\n",
      "    ('ha', 0.08261923847720276),\n",
      "    ('student', 0.08261923847720276),\n",
      "    ('support', 0.08261923847720276),\n",
      "    ('institution', 0.08261923847720276),\n",
      "    ('infrastructure', 0.08261923847720276),\n",
      "    ('course', 0.08261923847720276),\n",
      "    ('developer', 0.08261923847720276),\n",
      "    ('development', 0.08261923847720276),\n",
      "    ('tool', 0.08261923847720276),\n",
      "    ('india', 0.08261923847720276),\n",
      "    ('company', 0.08261923847720276),\n",
      "    ('provide', 0.08261923847720276),\n",
      "    ('cognitive', 0.08261923847720276),\n",
      "    ('learning', 0.08261923847720276),\n",
      "    ('technology', 0.08261923847720276),\n",
      "    ('industry', 0.08261923847720276),\n",
      "    ('job', 0.08261923847720276),\n",
      "    ('tomorrow', 0.08261923847720276),\n",
      "    ('require', 0.08261923847720276),\n",
      "    ('launched', 0.04130961923860138),\n",
      "    ('empower', 0.04130961923860138),\n",
      "    ('next', 0.04130961923860138),\n",
      "    ('generation', 0.04130961923860138),\n",
      "    ('envisioned', 0.04130961923860138),\n",
      "    ('three-year', 0.04130961923860138),\n",
      "    ('collaborative', 0.04130961923860138),\n",
      "    ('around', 0.04130961923860138),\n",
      "    ('100', 0.04130961923860138),\n",
      "    ('content', 0.04130961923860138),\n",
      "    ('curriculum', 0.04130961923860138),\n",
      "    ('give', 0.04130961923860138),\n",
      "    ('access', 0.04130961923860138),\n",
      "    ('part', 0.04130961923860138),\n",
      "    ('palo', 0.04130961923860138),\n",
      "    ('alto', 0.04130961923860138),\n",
      "    ('giant', 0.04130961923860138),\n",
      "    ('want', 0.04130961923860138),\n",
      "    ('expand', 0.04130961923860138),\n",
      "    ('reach', 0.04130961923860138),\n",
      "    ('planning', 0.04130961923860138),\n",
      "    ('strong', 0.04130961923860138),\n",
      "    ('ecosystem', 0.04130961923860138),\n",
      "    ('set', 0.04130961923860138),\n",
      "    ('core', 0.04130961923860138),\n",
      "    ('iot', 0.04130961923860138),\n",
      "    ('hub', 0.04130961923860138),\n",
      "    ('selected', 0.04130961923860138),\n",
      "    ('campus', 0.04130961923860138),\n",
      "    ('bot', 0.04130961923860138),\n",
      "    ('machine', 0.04130961923860138),\n",
      "    ('according', 0.04130961923860138),\n",
      "    ('mark', 0.04130961923860138),\n",
      "    ('smith', 0.04130961923860138),\n",
      "    ('country', 0.04130961923860138),\n",
      "    ('manager', 0.04130961923860138),\n",
      "    ('said', 0.04130961923860138),\n",
      "    ('``', 0.04130961923860138),\n",
      "    ('defining', 0.04130961923860138),\n",
      "    ('time', 0.04130961923860138),\n",
      "    ('transforming', 0.04130961923860138),\n",
      "    ('life', 0.04130961923860138),\n",
      "    ('different', 0.04130961923860138),\n",
      "    ('skillset', 0.04130961923860138),\n",
      "    ('collaboration', 0.04130961923860138),\n",
      "    ('training', 0.04130961923860138),\n",
      "    ('working', 0.04130961923860138),\n",
      "    ('’', 0.04130961923860138),\n",
      "    ('become', 0.04130961923860138),\n",
      "    ('critical', 0.04130961923860138),\n",
      "    ('ever', 0.04130961923860138),\n",
      "    ('educational', 0.04130961923860138),\n",
      "    ('integrate', 0.04130961923860138),\n",
      "    ('new', 0.04130961923860138),\n",
      "    ('ramp', 0.04130961923860138),\n",
      "    ('institutional', 0.04130961923860138),\n",
      "    ('set-up', 0.04130961923860138),\n",
      "    ('capability', 0.04130961923860138),\n",
      "    ('among', 0.04130961923860138),\n",
      "    ('educator', 0.04130961923860138),\n",
      "    ('educate', 0.04130961923860138),\n",
      "    (\"''\", 0.04130961923860138),\n",
      "    ('aim', 0.04130961923860138),\n",
      "    ('in-depth', 0.04130961923860138),\n",
      "    ('understanding', 0.04130961923860138),\n",
      "    ('developing', 0.04130961923860138),\n",
      "    ('intelligent', 0.04130961923860138),\n",
      "    ('connected', 0.04130961923860138),\n",
      "    ('solution', 0.04130961923860138),\n",
      "    ('application', 0.04130961923860138),\n",
      "    ('across', 0.04130961923860138),\n",
      "    ('earlier', 0.04130961923860138),\n",
      "    ('april', 0.04130961923860138),\n",
      "    ('year', 0.04130961923860138),\n",
      "    ('advanced', 0.04130961923860138),\n",
      "    ('track', 0.04130961923860138),\n",
      "    ('open', 0.04130961923860138),\n",
      "    ('public', 0.04130961923860138),\n",
      "    ('wa', 0.04130961923860138),\n",
      "    ('developed', 0.04130961923860138),\n",
      "    ('ready', 0.04130961923860138),\n",
      "    ('programmer', 0.04130961923860138),\n",
      "    ('wanted', 0.04130961923860138),\n",
      "    ('hone', 0.04130961923860138),\n",
      "    ('data', 0.04130961923860138),\n",
      "    ('science', 0.04130961923860138),\n",
      "    ('series', 0.04130961923860138),\n",
      "    ('online', 0.04130961923860138),\n",
      "    ('featured', 0.04130961923860138),\n",
      "    ('hands-on', 0.04130961923860138),\n",
      "    ('lab', 0.04130961923860138),\n",
      "    ('expert', 0.04130961923860138),\n",
      "    ('instructor', 0.04130961923860138),\n",
      "    ('well', 0.04130961923860138),\n",
      "    ('included', 0.04130961923860138),\n",
      "    ('developer-focused', 0.04130961923860138),\n",
      "    ('school', 0.04130961923860138),\n",
      "    ('provided', 0.04130961923860138),\n",
      "    ('bunch', 0.04130961923860138),\n",
      "    ('asset', 0.04130961923860138),\n",
      "    ('help', 0.04130961923860138)]\n"
     ]
    }
   ],
   "source": [
    "print(\"important_words=\")\n",
    "pp.pprint(summarised_result['important_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jQLb4Sqfc0E"
   },
   "source": [
    "### 4. Summarising text in python using a variation of TF-IDF method\n",
    "\n",
    "\n",
    "Inspired by Shivangi Sareen from the posts:\n",
    "[Summarise Text with TFIDF in Python 1](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
    "\n",
    "We have to break the text into sentences and tokens, ***we do not remove stop-words*** but do remove special characters. Tokenise words, calculate word TF and IDF frequencies to determine if a word is important on the corpus, using the TF-IDF technique. And then based on the average score method filter out only those sentences that meet the criteria.\n",
    "\n",
    "We could also use the (average score + 1.5 * std dev) or (average score + 3 * std dev), depending on the size of the target documents to summarise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWAPOQyt3w87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 0.011623382568359375\n",
      "('summarised_text=Earlier in April this year, the company announced SmartSoft '\n",
      " 'Corp. Advanced Program In AI as a learning track open to the public. That’s '\n",
      " 'why it has become more critical than ever for educational institutions to '\n",
      " 'integrate new cloud and AI technologies. According to Mark Smith, Country AI '\n",
      " 'Manager, SmartSoft Corp. India, said, \"With AI being the defining technology '\n",
      " 'of our time, it is transforming lives and industry and the jobs of tomorrow '\n",
      " 'will require a different skillset. In an attempt to build an AI-ready '\n",
      " 'workforce, SmartSoft Corp. announced Smart Colab Program which has been '\n",
      " 'launched to empower the next generation of students with AI-ready skills. '\n",
      " 'The program is an attempt to ramp up the institutional set-up and build '\n",
      " 'capabilities among the educators to educate the workforce of tomorrow.\"')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf-ignore-stopwords\")\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scored_documents=\n",
      "[   {   'doc_id': 10,\n",
      "        'sent_score': 1.5890269151739729,\n",
      "        'sentence': 'Earlier in April this year, the company announced '\n",
      "                    'SmartSoft Corp. Advanced Program In AI as a learning '\n",
      "                    'track open to the public.'},\n",
      "    {   'doc_id': 7,\n",
      "        'sent_score': 1.285959013388815,\n",
      "        'sentence': 'That’s why it has become more critical than ever for '\n",
      "                    'educational institutions to integrate new cloud and AI '\n",
      "                    'technologies.'},\n",
      "    {   'doc_id': 5,\n",
      "        'sent_score': 1.0194666026064951,\n",
      "        'sentence': 'According to Mark Smith, Country AI Manager, SmartSoft '\n",
      "                    'Corp. India, said, \"With AI being the defining technology '\n",
      "                    'of our time, it is transforming lives and industry and '\n",
      "                    'the jobs of tomorrow will require a different skillset.'},\n",
      "    {   'doc_id': 1,\n",
      "        'sent_score': 0.9895530969743801,\n",
      "        'sentence': 'In an attempt to build an AI-ready workforce, SmartSoft '\n",
      "                    'Corp. announced Smart Colab Program which has been '\n",
      "                    'launched to empower the next generation of students with '\n",
      "                    'AI-ready skills.'},\n",
      "    {   'doc_id': 8,\n",
      "        'sent_score': 0.8573060089258766,\n",
      "        'sentence': 'The program is an attempt to ramp up the institutional '\n",
      "                    'set-up and build capabilities among the educators to '\n",
      "                    'educate the workforce of tomorrow.\"'},\n",
      "    {   'doc_id': 2,\n",
      "        'sent_score': 0.5921245466315593,\n",
      "        'sentence': 'Envisioned as a three-year collaborative program, Smart '\n",
      "                    'Colab Program will support around 100 institutions with '\n",
      "                    'AI infrastructure, course content and curriculum, '\n",
      "                    'developer support, development tools and give students '\n",
      "                    'access to cloud and AI services.'},\n",
      "    {   'doc_id': 3,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'As part of the program, the Palo Alto giant which wants '\n",
      "                    'to expand its reach and is planning to build a strong '\n",
      "                    'developer ecosystem in India with the program will set up '\n",
      "                    'the core AI infrastructure and IoT Hub for the selected '\n",
      "                    'campuses.'},\n",
      "    {   'doc_id': 9,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'The program aims to build up the cognitive skills and '\n",
      "                    'in-depth understanding of developing intelligent cloud '\n",
      "                    'connected solutions for applications across industry.'},\n",
      "    {   'doc_id': 11,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'The program was developed to provide job ready skills to '\n",
      "                    'programmers who wanted to hone their skills in AI and '\n",
      "                    'data science with a series of online courses which '\n",
      "                    'featured hands-on labs and expert instructors as well.'},\n",
      "    {   'doc_id': 12,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'This program also included developer-focused AI school '\n",
      "                    'that provided a bunch of assets to help build AI skills.'},\n",
      "    {   'doc_id': 4,\n",
      "        'sent_score': 0.28905664484640153,\n",
      "        'sentence': 'The company will provide AI development tools and AI '\n",
      "                    'services such as SmartSoft Corp. Cognitive Services, Bot '\n",
      "                    'Services and Machine Learning Services.'},\n",
      "    {   'doc_id': 6,\n",
      "        'sent_score': 0.0870113769896297,\n",
      "        'sentence': 'This will require more collaborations and training and '\n",
      "                    'working with AI.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"scored_documents=\")\n",
    "pp.pprint(summarised_result['scored_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_JfZhqwfc0J"
   },
   "source": [
    "### Summarisation 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBXv8SZM3w9B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "better_nlp_summarisers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 }
}
