{
 "nbformat": 4,
 "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "better_nlp_summarisers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFyUOrS1fczL"
   },
   "source": [
    "# Better NLP\n",
    "\n",
    "This is a wrapper program/library that encapsulates a couple of NLP libraries that are popular among the AI and ML communities.\n",
    "\n",
    "Examples have been used to illustrate the usage as much as possible. Not all the APIs of the underlying libraries have been covered.\n",
    "\n",
    "The idea is to keep the API language as high-level as possible, so its easier to use and stays human-readable.\n",
    "\n",
    "Libraries / frameworks covered:\n",
    "\n",
    "- nltk [site](http://www.nltk.org/) | [docs](https://buildmedia.readthedocs.org/media/pdf/nltk/latest/nltk.pdf)\n",
    "- numpy [site](https://www.numpy.org/) | [docs](https://docs.scipy.org/doc/)\n",
    "- networkx [site](https://networkx.github.io/) | [docs](https://networkx.github.io/documentation/stable/index.html)\n",
    "\n",
    "See [https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp](https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp) for more details.\n",
    "\n",
    "### This notebook will demonstrate the below NLP features / functionalities, using the above mentioned libraries\n",
    "\n",
    "- Cosine similarity summarisation technique (extractive summarisation)\n",
    "- Vertex ranking algorithm summarisation technique\n",
    "- Build a simple text summarisation tool using NLTK\n",
    "- Summarisation 4 (TODO)\n",
    "- Summarisation 5 (TODO)\n",
    "\n",
    "_Summarisation can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMTAIhz53w8G"
   },
   "source": [
    "### Resources\n",
    "\n",
    "- [Understand Text Summarization and create your own summarizer in python](https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70) or [An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)\n",
    "- [Beyond bag of words: Using PyTextRank to find Phrases and Summarize text](https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5)\n",
    "- [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f)\n",
    "- [Summarise Text with TFIDF in Python 1/2](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2/2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
    "- [How to Make a Text Summarizer - Intro to Deep Learning #10 by Siraj Raval](https://www.youtube.com/watch?v=ogrJaOIuBx4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lre8GErufczN"
   },
   "source": [
    "#### Setup and installation ( optional )\n",
    "\n",
    "In case, this notebook is running in a local environment (Linux/MacOS) or _Google Colab_ environment and in case it does not have the necessary dependencies installed then please execute the steps in the next section.\n",
    "\n",
    "Otherwise, please SKIP to the **Install Spacy model ( NOT optional )** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5927
    },
    "colab_type": "code",
    "id": "QJuCUOMOfczO",
    "outputId": "018e9102-de25-4fbe-b5b3-919c58644f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "apt-utils is already the newest version (1.4.9).\n",
      "dpkg is already the newest version (1.18.25).\n",
      "dselect is already the newest version (1.18.25).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
      "OSTYPE=linux-gnu\n",
      "Library source found\n",
      "Detected OS: linux\n",
      "Please check if you fulfill the requirements mentioned in the README file.\n",
      "Hit:1 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Ign:2 http://deb.debian.org/debian stretch InRelease\n",
      "Hit:3 http://deb.debian.org/debian stretch-updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian stretch Release\n",
      "Hit:5 https://deb.nodesource.com/node_8.x stretch InRelease\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "curl is already the newest version (7.52.1-5+deb9u9).\n",
      "libswscale-dev is already the newest version (7:3.2.14-1~deb9u1).\n",
      "liblapack-dev is already the newest version (3.7.0-2).\n",
      "pkg-config is already the newest version (0.29-4+b1).\n",
      "wget is already the newest version (1.18-5+deb9u3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "vim is already the newest version (2:8.0.0197-4+deb9u3).\n",
      "zip is already the newest version (3.0-11+b1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
      "\n",
      "## Installing the NodeSource Node.js 8.x LTS Carbon repo...\n",
      "\n",
      "\n",
      "## Populating apt-get cache...\n",
      "\n",
      "+ apt-get update\n",
      "Ign:1 http://deb.debian.org/debian stretch InRelease\n",
      "Hit:2 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Hit:3 http://deb.debian.org/debian stretch-updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian stretch Release\n",
      "Hit:5 https://deb.nodesource.com/node_8.x stretch InRelease\n",
      "Reading package lists...\n",
      "\n",
      "## Confirming \"stretch\" is supported...\n",
      "\n",
      "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_8.x/dists/stretch/Release'\n",
      "\n",
      "## Adding the NodeSource signing key to your keyring...\n",
      "\n",
      "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | apt-key add -\n",
      "OK\n",
      "\n",
      "## Creating apt sources list file for the NodeSource Node.js 8.x LTS Carbon repo...\n",
      "\n",
      "+ echo 'deb https://deb.nodesource.com/node_8.x stretch main' > /etc/apt/sources.list.d/nodesource.list\n",
      "+ echo 'deb-src https://deb.nodesource.com/node_8.x stretch main' >> /etc/apt/sources.list.d/nodesource.list\n",
      "\n",
      "## Running `apt-get update` for you...\n",
      "\n",
      "+ apt-get update\n",
      "Ign:1 http://deb.debian.org/debian stretch InRelease\n",
      "Hit:2 http://security.debian.org/debian-security stretch/updates InRelease\n",
      "Hit:3 http://deb.debian.org/debian stretch-updates InRelease\n",
      "Hit:4 http://deb.debian.org/debian stretch Release\n",
      "Hit:5 https://deb.nodesource.com/node_8.x stretch InRelease\n",
      "Reading package lists...\n",
      "\n",
      "## Run `sudo apt-get install -y nodejs` to install Node.js 8.x LTS Carbon and npm\n",
      "## You may also need development tools to build native addons:\n",
      "     sudo apt-get install gcc g++ make\n",
      "## To install the Yarn package manager, run:\n",
      "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\n",
      "     echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
      "     sudo apt-get update && sudo apt-get install yarn\n",
      "\n",
      "\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "nodejs is already the newest version (8.16.1-1nodesource1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 60 not upgraded.\n",
      "Install components using python and pip\n",
      "/usr/bin/npm -> /usr/lib/node_modules/npm/bin/npm-cli.js\n",
      "/usr/bin/npx -> /usr/lib/node_modules/npm/bin/npx-cli.js\n",
      "+ npm@6.12.0\n",
      "updated 1 package in 40.539s\n",
      "6.12.0\n",
      "Python 3.7.3\n",
      "pip 19.0.3 from /usr/local/lib/python3.7/site-packages/pip (python 3.7)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/site-packages (2.1.4)\n",
      "Requirement already satisfied: textacy in /usr/local/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: pytextrank in /root/.local/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: nltk in /root/.local/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/site-packages (from spacy) (0.0.5)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.7/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.16.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.7/site-packages (from spacy) (7.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/site-packages (from textacy) (2.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from textacy) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn<0.21.0,>=0.17.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.20.3)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.9.0.1)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.7/site-packages (from textacy) (4.32.1)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/site-packages (from textacy) (0.13.2)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.7/site-packages (from textacy) (3.1.0)\n",
      "Requirement already satisfied: pyphen>=0.9.4 in /usr/local/lib/python3.7/site-packages (from textacy) (0.9.5)\n",
      "Requirement already satisfied: statistics in /root/.local/lib/python3.7/site-packages (from pytextrank) (1.0.3.5)\n",
      "Requirement already satisfied: datasketch in /root/.local/lib/python3.7/site-packages (from pytextrank) (1.4.3)\n",
      "Requirement already satisfied: graphviz in /root/.local/lib/python3.7/site-packages (from pytextrank) (0.10.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (0.15.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (19.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (40.8.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/site-packages (from networkx>=1.11->textacy) (4.4.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: docutils>=0.3 in /root/.local/lib/python3.7/site-packages (from statistics->pytextrank) (0.14)\n",
      "Requirement already satisfied: redis>=2.10.0 in /root/.local/lib/python3.7/site-packages (from datasketch->pytextrank) (3.2.1)\n",
      "Requirement already satisfied: jupyterlab in /root/.local/lib/python3.7/site-packages (0.35.6)\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /root/.local/lib/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: jupyterlab-server<0.3.0,>=0.2.0 in /root/.local/lib/python3.7/site-packages (from jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.3.1 in /root/.local/lib/python3.7/site-packages (from jupyterlab) (5.7.8)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/site-packages (from pandas) (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /root/.local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /root/.local/lib/python3.7/site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/.local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /root/.local/lib/python3.7/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /root/.local/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: jsonschema>=2.6.0 in /usr/local/lib/python3.7/site-packages (from jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (3.0.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.3.2)\n",
      "Requirement already satisfied: jinja2 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (2.10.1)\n",
      "Requirement already satisfied: ipykernel in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.2.4)\n",
      "Requirement already satisfied: Send2Trash in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: nbformat in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (6.0.2)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.8.2)\n",
      "Requirement already satisfied: prometheus-client in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: nbconvert in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (5.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (18.0.1)\n",
      "Requirement already satisfied: ipython-genutils in /root/.local/lib/python3.7/site-packages (from notebook>=4.3.1->jupyterlab) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6.0->jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (0.15.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6.0->jupyterlab-server<0.3.0,>=0.2.0->jupyterlab) (19.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from traitlets>=4.2.1->notebook>=4.3.1->jupyterlab) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /root/.local/lib/python3.7/site-packages (from jinja2->notebook>=4.3.1->jupyterlab) (1.1.1)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /root/.local/lib/python3.7/site-packages (from ipykernel->notebook>=4.3.1->jupyterlab) (7.5.0)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /root/.local/lib/python3.7/site-packages (from terminado>=0.8.1->notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.8.4)\n",
      "Requirement already satisfied: bleach in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (3.1.0)\n",
      "Requirement already satisfied: defusedxml in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.6.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (1.4.2)\n",
      "Requirement already satisfied: testpath in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (0.4.2)\n",
      "Requirement already satisfied: pygments in /root/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.3.1->jupyterlab) (2.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (2.0.9)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (4.7.0)\n",
      "Requirement already satisfied: backcall in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.13.3)\n",
      "Requirement already satisfied: pickleshare in /root/.local/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.7.5)\n",
      "Requirement already satisfied: webencodings in /root/.local/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.3.1->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: wcwidth in /root/.local/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in /root/.local/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook>=4.3.1->jupyterlab) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 20 ms, total: 40 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "apt-get install apt-utils dselect dpkg\n",
    "\n",
    "echo \"OSTYPE=$OSTYPE\"\n",
    "if [[ \"$OSTYPE\" == \"cygwin\" ]] || [[ \"$OSTYPE\" == \"msys\" ]] ; then\n",
    "    echo \"Windows or Windows-like environment detected, script not tested, and may not work.\"\n",
    "    echo \"Try installing the components mention in the install-[ostype].sh scripts manually.\"\n",
    "    echo \"Or try running under CGYWIN or git-bash.\"\n",
    "    echo \"If successfully installed, please contribute back with the solution via a pull request, to https://github.com/neomatrix369/awesome-ai-ml-dl/\"\n",
    "    echo \"Please give the file a good name, i.e. install-windows.sh or install-windows.bat depending on what kind of script you end up writing\"\n",
    "    exit 0\n",
    "elif [[ \"$OSTYPE\" == \"linux-gnu\" ]] || [[ \"$OSTYPE\" == \"linux\" ]]; then\n",
    "    TARGET_OS=\"linux\"\n",
    "else\n",
    "    TARGET_OS=\"macos\"\n",
    "fi\n",
    "\n",
    "if [[ -e ../../library/org/neomatrix369 ]]; then\n",
    "  echo \"Library source found\"\n",
    "  \n",
    "  cd ../../build\n",
    "  \n",
    "  echo \"Detected OS: ${TARGET_OS}\"\n",
    "  ./install-${TARGET_OS}.sh || true\n",
    "else\n",
    "  if [[ -e awesome-ai-ml-dl/examples/better-nlp/library ]]; then\n",
    "     echo \"Library source found\"\n",
    "  else\n",
    "     git clone \"https://github.com/neomatrix369/awesome-ai-ml-dl\"\n",
    "  fi\n",
    "\n",
    "  echo \"Library source exists\"\n",
    "  cd awesome-ai-ml-dl/examples/better-nlp/build\n",
    "\n",
    "  echo \"Detected OS: ${TARGET_OS}\"\n",
    "  ./install-${TARGET_OS}.sh || true \n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPLdwvt63w8R"
   },
   "source": [
    "#### Install Spacy model ( NOT optional )\n",
    "\n",
    "Install the large English language model for spaCy - will be needed for the examples in this notebooks.\n",
    "\n",
    "**Note:** from observation it appears that spaCy model should be installed towards the end of the installation process, it avoid errors when running programs using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dJrJ54a3w8S",
    "outputId": "3da4300b-89a8-43e3-e989-fa7859ecdfc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /usr/local/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n",
      "\u001b[38;5;1m✘ Link 'en' already exists\u001b[0m\n",
      "To overwrite an existing link, use the --force flag\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 20 ms, total: 20 ms\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "python -m spacy download en_core_web_lg\n",
    "python -m spacy link en_core_web_lg en || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwXgEdM8oeUv"
   },
   "source": [
    "## Examples of various summarisation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AX1pZlKofczb"
   },
   "source": [
    "### 1. Cosine similarity summarisation technique (extractive summarisation)\n",
    "\n",
    "**Abstractive Summarization:** Abstractive methods select words based on semantic understanding, even those words did not appear in the source documents. It aims at producing important material in a new way. They interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text.\n",
    "\n",
    "**Flow:** Input document → understand context → semantics → create own summary\n",
    "\n",
    "**Extractive Summarization:** Extractive methods attempt to summarize articles by selecting a subset of words that retain the most important points.\n",
    "\n",
    "**Flow:** Input document → sentences similarity → weight sentences → select sentences with higher rank\n",
    "\n",
    "**Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. Angle will be 0 if sentences are similar and tend towards 90 as they begin to differ.\n",
    "\n",
    "Inspired by Praveen Dubey the author of https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
    "\n",
    "or see [Understand Text Summarization and create your own summarizer in python](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yjan7P5_3w8Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n",
      "This version of Python is 64 bits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../library')\n",
    "sys.path.insert(0, './awesome-ai-ml-dl/examples/better-nlp/library')\n",
    "\n",
    "from org.neomatrix369.better_nlp import BetterNLP\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpjkLRHe3w8c"
   },
   "outputs": [],
   "source": [
    "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object\n",
    "generic_text=\"\"\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vkYIGs13w8f",
    "outputId": "083398af-ca33-4c1c-f892-e1567de17931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 0.11545395851135254\n",
      "('summarised_text=The company will provide AI development tools and Azure AI '\n",
      " 'services such as SmartSoft Corp. Envisioned as a three-year collaborative '\n",
      " 'program, Smart Colab Program will support around 100 institutions with AI '\n",
      " 'infrastructure, course content and curriculum, developer support, '\n",
      " 'development tools and give students access to cloud and AI services. '\n",
      " 'According to Mark Smith, Country AI Manager, SmartSoft Corp. Advanced '\n",
      " 'Program In AI as a learning track open to the public. This will require more '\n",
      " 'collaborations and training and working with AI')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text)\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranked_sentences=\n",
      "[   (   0.1344483475843947,\n",
      "        [   'The',\n",
      "            'company',\n",
      "            'will',\n",
      "            'provide',\n",
      "            'AI',\n",
      "            'development',\n",
      "            'tools',\n",
      "            'and',\n",
      "            'Azure',\n",
      "            'AI',\n",
      "            'services',\n",
      "            'such',\n",
      "            'as',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.10424930788926609,\n",
      "        [   'Envisioned',\n",
      "            'as',\n",
      "            'a',\n",
      "            'three-year',\n",
      "            'collaborative',\n",
      "            'program,',\n",
      "            'Smart',\n",
      "            'Colab',\n",
      "            'Program',\n",
      "            'will',\n",
      "            'support',\n",
      "            'around',\n",
      "            '100',\n",
      "            'institutions',\n",
      "            'with',\n",
      "            'AI',\n",
      "            'infrastructure,',\n",
      "            'course',\n",
      "            'content',\n",
      "            'and',\n",
      "            'curriculum,',\n",
      "            'developer',\n",
      "            'support,',\n",
      "            'development',\n",
      "            'tools',\n",
      "            'and',\n",
      "            'give',\n",
      "            'students',\n",
      "            'access',\n",
      "            'to',\n",
      "            'cloud',\n",
      "            'and',\n",
      "            'AI',\n",
      "            'services']),\n",
      "    (   0.09018155296051672,\n",
      "        [   'According',\n",
      "            'to',\n",
      "            'Mark',\n",
      "            'Smith,',\n",
      "            'Country',\n",
      "            'AI',\n",
      "            'Manager,',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.08757177858592194,\n",
      "        [   'Advanced',\n",
      "            'Program',\n",
      "            'In',\n",
      "            'AI',\n",
      "            'as',\n",
      "            'a',\n",
      "            'learning',\n",
      "            'track',\n",
      "            'open',\n",
      "            'to',\n",
      "            'the',\n",
      "            'public']),\n",
      "    (   0.07052989184442125,\n",
      "        [   'This',\n",
      "            'will',\n",
      "            'require',\n",
      "            'more',\n",
      "            'collaborations',\n",
      "            'and',\n",
      "            'training',\n",
      "            'and',\n",
      "            'working',\n",
      "            'with',\n",
      "            'AI']),\n",
      "    (   0.0690373780667743,\n",
      "        [   'In',\n",
      "            'an',\n",
      "            'attempt',\n",
      "            'to',\n",
      "            'build',\n",
      "            'an',\n",
      "            'AI-ready',\n",
      "            'workforce,',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.06361746932728805,\n",
      "        [   'The',\n",
      "            'program',\n",
      "            'was',\n",
      "            'developed',\n",
      "            'to',\n",
      "            'provide',\n",
      "            'job',\n",
      "            'ready',\n",
      "            'skills',\n",
      "            'to',\n",
      "            'programmers',\n",
      "            'who',\n",
      "            'wanted',\n",
      "            'to',\n",
      "            'hone',\n",
      "            'their',\n",
      "            'skills',\n",
      "            'in',\n",
      "            'AI',\n",
      "            'and',\n",
      "            'data',\n",
      "            'science',\n",
      "            'with',\n",
      "            'a',\n",
      "            'series',\n",
      "            'of',\n",
      "            'online',\n",
      "            'courses',\n",
      "            'which',\n",
      "            'featured',\n",
      "            'hands-on',\n",
      "            'labs',\n",
      "            'and',\n",
      "            'expert',\n",
      "            'instructors',\n",
      "            'as',\n",
      "            'well']),\n",
      "    (   0.06340739923819796,\n",
      "        [   'As',\n",
      "            'part',\n",
      "            'of',\n",
      "            'the',\n",
      "            'program,',\n",
      "            'the',\n",
      "            'Palo',\n",
      "            'Alto',\n",
      "            'giant',\n",
      "            'which',\n",
      "            'wants',\n",
      "            'to',\n",
      "            'expand',\n",
      "            'its',\n",
      "            'reach',\n",
      "            'and',\n",
      "            'is',\n",
      "            'planning',\n",
      "            'to',\n",
      "            'build',\n",
      "            'a',\n",
      "            'strong',\n",
      "            'developer',\n",
      "            'ecosystem',\n",
      "            'in',\n",
      "            'India',\n",
      "            'with',\n",
      "            'the',\n",
      "            'program',\n",
      "            'will',\n",
      "            'set',\n",
      "            'up',\n",
      "            'the',\n",
      "            'core',\n",
      "            'AI',\n",
      "            'infrastructure',\n",
      "            'and',\n",
      "            'IoT',\n",
      "            'Hub',\n",
      "            'for',\n",
      "            'the',\n",
      "            'selected',\n",
      "            'campuses']),\n",
      "    (   0.06182256247688828,\n",
      "        [   'The',\n",
      "            'program',\n",
      "            'is',\n",
      "            'an',\n",
      "            'attempt',\n",
      "            'to',\n",
      "            'ramp',\n",
      "            'up',\n",
      "            'the',\n",
      "            'institutional',\n",
      "            'set-up',\n",
      "            'and',\n",
      "            'build',\n",
      "            'capabilities',\n",
      "            'among',\n",
      "            'the',\n",
      "            'educators',\n",
      "            'to',\n",
      "            'educate',\n",
      "            'the',\n",
      "            'workforce',\n",
      "            'of',\n",
      "            'tomorrow.\"',\n",
      "            'The',\n",
      "            'program',\n",
      "            'aims',\n",
      "            'to',\n",
      "            'build',\n",
      "            'up',\n",
      "            'the',\n",
      "            'cognitive',\n",
      "            'skills',\n",
      "            'and',\n",
      "            'in-depth',\n",
      "            'understanding',\n",
      "            'of',\n",
      "            'developing',\n",
      "            'intelligent',\n",
      "            'cloud',\n",
      "            'connected',\n",
      "            'solutions',\n",
      "            'for',\n",
      "            'applications',\n",
      "            'across',\n",
      "            'industry']),\n",
      "    (   0.056811587785028955,\n",
      "        [   'That’s',\n",
      "            'why',\n",
      "            'it',\n",
      "            'has',\n",
      "            'become',\n",
      "            'more',\n",
      "            'critical',\n",
      "            'than',\n",
      "            'ever',\n",
      "            'for',\n",
      "            'educational',\n",
      "            'institutions',\n",
      "            'to',\n",
      "            'integrate',\n",
      "            'new',\n",
      "            'cloud',\n",
      "            'and',\n",
      "            'AI',\n",
      "            'technologies']),\n",
      "    (   0.05680495283402265,\n",
      "        [   'announced',\n",
      "            'Smart',\n",
      "            'Colab',\n",
      "            'Program',\n",
      "            'which',\n",
      "            'has',\n",
      "            'been',\n",
      "            'launched',\n",
      "            'to',\n",
      "            'empower',\n",
      "            'the',\n",
      "            'next',\n",
      "            'generation',\n",
      "            'of',\n",
      "            'students',\n",
      "            'with',\n",
      "            'AI-ready',\n",
      "            'skills']),\n",
      "    (   0.05661355569967891,\n",
      "        [   'Earlier',\n",
      "            'in',\n",
      "            'April',\n",
      "            'this',\n",
      "            'year,',\n",
      "            'the',\n",
      "            'company',\n",
      "            'announced',\n",
      "            'SmartSoft',\n",
      "            'Corp']),\n",
      "    (   0.051605080775006446,\n",
      "        [   'India,',\n",
      "            'said,',\n",
      "            '\"With',\n",
      "            'AI',\n",
      "            'being',\n",
      "            'the',\n",
      "            'defining',\n",
      "            'technology',\n",
      "            'of',\n",
      "            'our',\n",
      "            'time,',\n",
      "            'it',\n",
      "            'is',\n",
      "            'transforming',\n",
      "            'lives',\n",
      "            'and',\n",
      "            'industry',\n",
      "            'and',\n",
      "            'the',\n",
      "            'jobs',\n",
      "            'of',\n",
      "            'tomorrow',\n",
      "            'will',\n",
      "            'require',\n",
      "            'a',\n",
      "            'different',\n",
      "            'skillset']),\n",
      "    (   0.0332991349325937,\n",
      "        [   'Cognitive',\n",
      "            'Services,',\n",
      "            'Bot',\n",
      "            'Services',\n",
      "            'and',\n",
      "            'Machine',\n",
      "            'Learning',\n",
      "            'Services'])]\n"
     ]
    }
   ],
   "source": [
    "print(\"ranked_sentences=\") \n",
    "pp.pprint(summarised_result['ranked_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7A3p05vfcz0"
   },
   "source": [
    "### Vertex ranking algorithm summarisation technique\n",
    "\n",
    "Using PyTextRank to find Phrases and Summarize text: Multi-word Phrase Extraction and Sentence Extraction for Summarization\n",
    "\n",
    "Inspired by the author of https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5 \n",
    "(Notebook: https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb)\n",
    "\n",
    "Another resource to take a look at: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzEgIzdi3w8j",
    "outputId": "dc771fd8-16ef-487d-f23c-9236d62b97d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sed: can't read /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py: No such file or directory\n",
      "sed: can't read /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70 ms, sys: 20 ms, total: 90 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash\n",
    "\n",
    "sed -i 's/graph.edge\\[/graph.adj\\[/g' /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "sed -i 's/, parse=True//g' /usr/local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "\n",
    "sed -i 's/graph.edge\\[/graph.adj\\[/g' ~/.local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true\n",
    "sed -i 's/, parse=True//g' ~/.local/lib/python3.7/site-packages/pytextrank/pytextrank.py || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0by6FnE3w8n"
   },
   "outputs": [],
   "source": [
    "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w44pghUL3w8r",
    "outputId": "885ba416-64b3-48ef-b27c-9b66f2ca069f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to incompatibility between spacy and pytextrank, we are unable to generate a summary\n"
     ]
    }
   ],
   "source": [
    "source_file='source.json'\n",
    "source_json_content='{\"id\":\"777\", \"text\":\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, ''With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That''s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.'' The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"}'\n",
    "with open(source_file, 'w') as f:\n",
    "    f.write(\"%s\" % source_json_content)\n",
    "try:\n",
    "    summarised_result = betterNLP.summarise(source_file, method=\"pytextrank\")\n",
    "\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "    print(\"summarised_text=\",summarised_result['summarised_text'])\n",
    "    print(\"token_ranks=\",summarised_result['token_ranks'])\n",
    "    print(\"key_phrases=\",summarised_result['key_phrases'])\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    betterNLP.show_graph(summarised_result[\"graph\"])\n",
    "except:\n",
    "    print(\"Due to incompatibility between spacy and pytextrank, we are unable to generate a summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2kyTI6bfcz7"
   },
   "source": [
    "### 3. Build a simple text summarisation tool using NLTK\n",
    "\n",
    "Inspired by Wilame Lima Vallantin, the author of [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f).\n",
    "\n",
    "We have to break the text into sentences and tokens, remove stop-words. Tokenise words, calculate word frequency to determine if a word is important on the corpus, using the TF-IDF technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOqeUNkg3w82",
    "outputId": "bfe3346d-713d-4114-b0e8-1a8539895072"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 5.3451220989227295\n",
      "summarised_text=\n",
      "[   'Envisioned as a three-year collaborative program, Smart Colab Program '\n",
      "    'will support around 100 institutions with AI infrastructure, course '\n",
      "    'content and curriculum, developer support, development tools and give '\n",
      "    'students access to cloud and AI services.',\n",
      "    'As part of the program, the Palo Alto giant which wants to expand its '\n",
      "    'reach and is planning to build a strong developer ecosystem in India with '\n",
      "    'the program will set up the core AI infrastructure and IoT Hub for the '\n",
      "    'selected campuses.']\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'al', 'au', 'bi', 'could', 'de', 'diesis', 'doe', 'dy', 'e', 'ha', 'might', 'mus', 'must', \"n't\", 'need', 'sha', 'un', 'wa', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf\")\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "print(\"summarised_text=\")\n",
    "pp.pprint(summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "important_words=\n",
      "[   ('ai', 0.5365674232184474),\n",
      "    ('program', 0.412744171706498),\n",
      "    ('build', 0.206372085853249),\n",
      "    ('skill', 0.206372085853249),\n",
      "    ('service', 0.206372085853249),\n",
      "    ('smartsoft', 0.1650976686825992),\n",
      "    ('corp.', 0.1650976686825992),\n",
      "    ('cloud', 0.12382325151194941),\n",
      "    ('attempt', 0.0825488343412996),\n",
      "    ('ai-ready', 0.0825488343412996),\n",
      "    ('workforce', 0.0825488343412996),\n",
      "    ('announced', 0.0825488343412996),\n",
      "    ('smart', 0.0825488343412996),\n",
      "    ('colab', 0.0825488343412996),\n",
      "    ('ha', 0.0825488343412996),\n",
      "    ('student', 0.0825488343412996),\n",
      "    ('support', 0.0825488343412996),\n",
      "    ('institution', 0.0825488343412996),\n",
      "    ('infrastructure', 0.0825488343412996),\n",
      "    ('course', 0.0825488343412996),\n",
      "    ('developer', 0.0825488343412996),\n",
      "    ('development', 0.0825488343412996),\n",
      "    ('tool', 0.0825488343412996),\n",
      "    ('india', 0.0825488343412996),\n",
      "    ('company', 0.0825488343412996),\n",
      "    ('provide', 0.0825488343412996),\n",
      "    ('cognitive', 0.0825488343412996),\n",
      "    ('learning', 0.0825488343412996),\n",
      "    ('technology', 0.0825488343412996),\n",
      "    ('industry', 0.0825488343412996),\n",
      "    ('job', 0.0825488343412996),\n",
      "    ('tomorrow', 0.0825488343412996),\n",
      "    ('require', 0.0825488343412996),\n",
      "    ('launched', 0.0412744171706498),\n",
      "    ('empower', 0.0412744171706498),\n",
      "    ('next', 0.0412744171706498),\n",
      "    ('generation', 0.0412744171706498),\n",
      "    ('envisioned', 0.0412744171706498),\n",
      "    ('three-year', 0.0412744171706498),\n",
      "    ('collaborative', 0.0412744171706498),\n",
      "    ('around', 0.0412744171706498),\n",
      "    ('100', 0.0412744171706498),\n",
      "    ('content', 0.0412744171706498),\n",
      "    ('curriculum', 0.0412744171706498),\n",
      "    ('give', 0.0412744171706498),\n",
      "    ('access', 0.0412744171706498),\n",
      "    ('part', 0.0412744171706498),\n",
      "    ('palo', 0.0412744171706498),\n",
      "    ('alto', 0.0412744171706498),\n",
      "    ('giant', 0.0412744171706498),\n",
      "    ('want', 0.0412744171706498),\n",
      "    ('expand', 0.0412744171706498),\n",
      "    ('reach', 0.0412744171706498),\n",
      "    ('planning', 0.0412744171706498),\n",
      "    ('strong', 0.0412744171706498),\n",
      "    ('ecosystem', 0.0412744171706498),\n",
      "    ('set', 0.0412744171706498),\n",
      "    ('core', 0.0412744171706498),\n",
      "    ('iot', 0.0412744171706498),\n",
      "    ('hub', 0.0412744171706498),\n",
      "    ('selected', 0.0412744171706498),\n",
      "    ('campus', 0.0412744171706498),\n",
      "    ('azure', 0.0412744171706498),\n",
      "    ('bot', 0.0412744171706498),\n",
      "    ('machine', 0.0412744171706498),\n",
      "    ('according', 0.0412744171706498),\n",
      "    ('mark', 0.0412744171706498),\n",
      "    ('smith', 0.0412744171706498),\n",
      "    ('country', 0.0412744171706498),\n",
      "    ('manager', 0.0412744171706498),\n",
      "    ('said', 0.0412744171706498),\n",
      "    ('``', 0.0412744171706498),\n",
      "    ('defining', 0.0412744171706498),\n",
      "    ('time', 0.0412744171706498),\n",
      "    ('transforming', 0.0412744171706498),\n",
      "    ('life', 0.0412744171706498),\n",
      "    ('different', 0.0412744171706498),\n",
      "    ('skillset', 0.0412744171706498),\n",
      "    ('collaboration', 0.0412744171706498),\n",
      "    ('training', 0.0412744171706498),\n",
      "    ('working', 0.0412744171706498),\n",
      "    ('’', 0.0412744171706498),\n",
      "    ('become', 0.0412744171706498),\n",
      "    ('critical', 0.0412744171706498),\n",
      "    ('ever', 0.0412744171706498),\n",
      "    ('educational', 0.0412744171706498),\n",
      "    ('integrate', 0.0412744171706498),\n",
      "    ('new', 0.0412744171706498),\n",
      "    ('ramp', 0.0412744171706498),\n",
      "    ('institutional', 0.0412744171706498),\n",
      "    ('set-up', 0.0412744171706498),\n",
      "    ('capability', 0.0412744171706498),\n",
      "    ('among', 0.0412744171706498),\n",
      "    ('educator', 0.0412744171706498),\n",
      "    ('educate', 0.0412744171706498),\n",
      "    (\"''\", 0.0412744171706498),\n",
      "    ('aim', 0.0412744171706498),\n",
      "    ('in-depth', 0.0412744171706498),\n",
      "    ('understanding', 0.0412744171706498),\n",
      "    ('developing', 0.0412744171706498),\n",
      "    ('intelligent', 0.0412744171706498),\n",
      "    ('connected', 0.0412744171706498),\n",
      "    ('solution', 0.0412744171706498),\n",
      "    ('application', 0.0412744171706498),\n",
      "    ('across', 0.0412744171706498),\n",
      "    ('earlier', 0.0412744171706498),\n",
      "    ('april', 0.0412744171706498),\n",
      "    ('year', 0.0412744171706498),\n",
      "    ('advanced', 0.0412744171706498),\n",
      "    ('track', 0.0412744171706498),\n",
      "    ('open', 0.0412744171706498),\n",
      "    ('public', 0.0412744171706498),\n",
      "    ('wa', 0.0412744171706498),\n",
      "    ('developed', 0.0412744171706498),\n",
      "    ('ready', 0.0412744171706498),\n",
      "    ('programmer', 0.0412744171706498),\n",
      "    ('wanted', 0.0412744171706498),\n",
      "    ('hone', 0.0412744171706498),\n",
      "    ('data', 0.0412744171706498),\n",
      "    ('science', 0.0412744171706498),\n",
      "    ('series', 0.0412744171706498),\n",
      "    ('online', 0.0412744171706498),\n",
      "    ('featured', 0.0412744171706498),\n",
      "    ('hands-on', 0.0412744171706498),\n",
      "    ('lab', 0.0412744171706498),\n",
      "    ('expert', 0.0412744171706498),\n",
      "    ('instructor', 0.0412744171706498),\n",
      "    ('well', 0.0412744171706498),\n",
      "    ('included', 0.0412744171706498),\n",
      "    ('developer-focused', 0.0412744171706498),\n",
      "    ('school', 0.0412744171706498),\n",
      "    ('provided', 0.0412744171706498),\n",
      "    ('bunch', 0.0412744171706498),\n",
      "    ('asset', 0.0412744171706498),\n",
      "    ('help', 0.0412744171706498)]\n"
     ]
    }
   ],
   "source": [
    "print(\"important_words=\")\n",
    "pp.pprint(summarised_result['important_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jQLb4Sqfc0E"
   },
   "source": [
    "### 4. Summarising text in python using a variation of TF-IDF method\n",
    "\n",
    "\n",
    "Inspired by Shivangi Sareen from the posts:\n",
    "[Summarise Text with TFIDF in Python 1](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
    "\n",
    "We have to break the text into sentences and tokens, ***we do not remove stop-words*** but do remove special characters. Tokenise words, calculate word TF and IDF frequencies to determine if a word is important on the corpus, using the TF-IDF technique. And then based on the average score method filter out only those sentences that meet the criteria.\n",
    "\n",
    "We could also use the (average score + 1.5 * std dev) or (average score + 3 * std dev), depending on the size of the target documents to summarise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWAPOQyt3w87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "summarisation_processing_time_in_secs= 0.06758618354797363\n",
      "('summarised_text=Earlier in April this year, the company announced SmartSoft '\n",
      " 'Corp. Advanced Program In AI as a learning track open to the public. That’s '\n",
      " 'why it has become more critical than ever for educational institutions to '\n",
      " 'integrate new cloud and AI technologies. According to Mark Smith, Country AI '\n",
      " 'Manager, SmartSoft Corp. India, said, \"With AI being the defining technology '\n",
      " 'of our time, it is transforming lives and industry and the jobs of tomorrow '\n",
      " 'will require a different skillset. In an attempt to build an AI-ready '\n",
      " 'workforce, SmartSoft Corp. announced Smart Colab Program which has been '\n",
      " 'launched to empower the next generation of students with AI-ready skills. '\n",
      " 'The program is an attempt to ramp up the institutional set-up and build '\n",
      " 'capabilities among the educators to educate the workforce of tomorrow.\"')\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf-ignore-stopwords\")\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
    "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scored_documents=\n",
      "[   {   'doc_id': 10,\n",
      "        'sent_score': 1.5890269151739729,\n",
      "        'sentence': 'Earlier in April this year, the company announced '\n",
      "                    'SmartSoft Corp. Advanced Program In AI as a learning '\n",
      "                    'track open to the public.'},\n",
      "    {   'doc_id': 7,\n",
      "        'sent_score': 1.285959013388815,\n",
      "        'sentence': 'That’s why it has become more critical than ever for '\n",
      "                    'educational institutions to integrate new cloud and AI '\n",
      "                    'technologies.'},\n",
      "    {   'doc_id': 5,\n",
      "        'sent_score': 1.0194666026064951,\n",
      "        'sentence': 'According to Mark Smith, Country AI Manager, SmartSoft '\n",
      "                    'Corp. India, said, \"With AI being the defining technology '\n",
      "                    'of our time, it is transforming lives and industry and '\n",
      "                    'the jobs of tomorrow will require a different skillset.'},\n",
      "    {   'doc_id': 1,\n",
      "        'sent_score': 0.9895530969743801,\n",
      "        'sentence': 'In an attempt to build an AI-ready workforce, SmartSoft '\n",
      "                    'Corp. announced Smart Colab Program which has been '\n",
      "                    'launched to empower the next generation of students with '\n",
      "                    'AI-ready skills.'},\n",
      "    {   'doc_id': 8,\n",
      "        'sent_score': 0.8573060089258766,\n",
      "        'sentence': 'The program is an attempt to ramp up the institutional '\n",
      "                    'set-up and build capabilities among the educators to '\n",
      "                    'educate the workforce of tomorrow.\"'},\n",
      "    {   'doc_id': 2,\n",
      "        'sent_score': 0.5921245466315593,\n",
      "        'sentence': 'Envisioned as a three-year collaborative program, Smart '\n",
      "                    'Colab Program will support around 100 institutions with '\n",
      "                    'AI infrastructure, course content and curriculum, '\n",
      "                    'developer support, development tools and give students '\n",
      "                    'access to cloud and AI services.'},\n",
      "    {   'doc_id': 3,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'As part of the program, the Palo Alto giant which wants '\n",
      "                    'to expand its reach and is planning to build a strong '\n",
      "                    'developer ecosystem in India with the program will set up '\n",
      "                    'the core AI infrastructure and IoT Hub for the selected '\n",
      "                    'campuses.'},\n",
      "    {   'doc_id': 9,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'The program aims to build up the cognitive skills and '\n",
      "                    'in-depth understanding of developing intelligent cloud '\n",
      "                    'connected solutions for applications across industry.'},\n",
      "    {   'doc_id': 11,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'The program was developed to provide job ready skills to '\n",
      "                    'programmers who wanted to hone their skills in AI and '\n",
      "                    'data science with a series of online courses which '\n",
      "                    'featured hands-on labs and expert instructors as well.'},\n",
      "    {   'doc_id': 12,\n",
      "        'sent_score': 0.3900792787747875,\n",
      "        'sentence': 'This program also included developer-focused AI school '\n",
      "                    'that provided a bunch of assets to help build AI skills.'},\n",
      "    {   'doc_id': 4,\n",
      "        'sent_score': 0.28905664484640153,\n",
      "        'sentence': 'The company will provide AI development tools and Azure '\n",
      "                    'AI services such as SmartSoft Corp. Cognitive Services, '\n",
      "                    'Bot Services and Machine Learning Services.'},\n",
      "    {   'doc_id': 6,\n",
      "        'sent_score': 0.0870113769896297,\n",
      "        'sentence': 'This will require more collaborations and training and '\n",
      "                    'working with AI.'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"scored_documents=\")\n",
    "pp.pprint(summarised_result['scored_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_JfZhqwfc0J"
   },
   "source": [
    "### Summarisation 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBXv8SZM3w9B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "better_nlp_summarisers.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
